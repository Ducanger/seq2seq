{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaConfig, RobertaModel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "model = RobertaModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "tmp = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = model.embeddings(torch.tensor([1],device=device)[None,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = model(torch.tensor([1],device=device)[None,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model.embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec.load(\"w2v128\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.key_to_index.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(model.wv.key_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "xx = numpy.zeros((5,128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx[:3] = model.wv['Equ', 'Ġpair', 'ĠCut']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv[['the', 'if', 'return']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaConfig, RobertaModel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "model = RobertaModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "x = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_ids=tokenizer.convert_tokens_to_ids(code_tokens)\n",
    "tokenizer.convert_ids_to_tokens(tokens_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "df = pd.read_parquet(f'data/cmg-data-processed.parquet', engine='fastparquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['index','label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)\n",
    "df['index'] = df['index'].apply(lambda x: x.replace('_file_fc_patch.csv',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('commit_message.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [print(el) for el in list(df['label'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('commit_message.txt','w+') as f:\n",
    "    f.write('\\n'.join(list(df['label'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "include/asm-ppc/seccomp.h\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "df = pd.read_parquet(f'data/cmg-data-processed.parquet', engine='fastparquet')\n",
    "df = df.sort_values(by=['extract_level'])\n",
    "result = list()\n",
    "for _, row in df.iterrows():\n",
    "    diff = list()\n",
    "    for l in row['diff'].splitlines():\n",
    "        if l.startswith('-') or l.startswith('+'):\n",
    "            diff.append(l)\n",
    "    doc = row['label'].split()\n",
    "    if row['old_path_file'] == row['new_path_file']:\n",
    "        file = row['new_path_file']\n",
    "    else:\n",
    "        if row['old_path_file'] is not None and row['new_path_file'] is not None:\n",
    "            file = row['old_path_file'] + ' SEP ' + row['new_path_file']\n",
    "        else:\n",
    "            file = row['old_path_file'] if row['old_path_file'] is not None else row['new_path_file']\n",
    "        print(file)\n",
    "    file = file+ ' SEP'\n",
    "    index = row['index'].replace('_file_fc_patch.csv','')\n",
    "    code = file + '\\n'.join(diff)\n",
    "    code = code.split()\n",
    "    result.append({'code_tokens':code,'docstring_tokens':doc,'index':index})\n",
    "import random \n",
    "random.shuffle(result)\n",
    "train, val,test = result[:17848],result[17848:20398],result[20398:]\n",
    "import json\n",
    "def dump_to_file(obj, file):\n",
    "    with open(file,'w+') as f:\n",
    "        for el in obj:\n",
    "            f.write(json.dumps(el)+'\\n')\n",
    "dump_to_file(train,'data/train.jsonl')\n",
    "dump_to_file(test,'data/test.jsonl')\n",
    "dump_to_file(val,'data/valid.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25498"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(df['index']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25498, 12)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "include/asm-ppc/seccomp.h\n"
     ]
    }
   ],
   "source": [
    "result = list()\n",
    "for _, row in df.iterrows():\n",
    "    diff = list()\n",
    "    for l in row['diff'].splitlines():\n",
    "        if l.startswith('-') or l.startswith('+'):\n",
    "            diff.append(l)\n",
    "    doc = row['label'].split()\n",
    "    if row['old_path_file'] == row['new_path_file']:\n",
    "        file = row['new_path_file']\n",
    "    else:\n",
    "        if row['old_path_file'] is not None and row['new_path_file'] is not None:\n",
    "            file = row['old_path_file'] + ' SEP ' + row['new_path_file']\n",
    "        else:\n",
    "            file = row['old_path_file'] if row['old_path_file'] is not None else row['new_path_file']\n",
    "        print(file)\n",
    "    file = file+ ' SEP'\n",
    "    index = row['index'].replace('_file_fc_patch.csv','')\n",
    "    code = file + '\\n'.join(diff)\n",
    "    code = code.split()\n",
    "    result.append({'code_tokens':code,'docstring_tokens':doc,'index':index})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val,test = result[:17848],result[17848:20398],result[20398:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def dump_to_file(obj, file):\n",
    "    with open(file,'w+') as f:\n",
    "        for el in obj:\n",
    "            f.write(json.dumps(el)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_to_file(train,'data/train.jsonl')\n",
    "dump_to_file(test,'data/test.jsonl')\n",
    "dump_to_file(val,'data/valid.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaTokenizer(name_or_path='microsoft/codebert-base', vocab_size=50265, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 32, 512])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
    "src = torch.rand(10, 32, 512)\n",
    "out = transformer_encoder(src)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.99-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m360.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.99\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "# spm.SentencePieceTrainer.train(input='test/botchan.txt', model_prefix='m', vocab_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = spm.SentencePieceProcessor(model_file='m.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an Embedding module containing 10 tensors of size 3\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "embedding = nn.Embedding(10, 3)\n",
    "# a batch of 2 samples of 4 indices each\n",
    "input = torch.LongTensor([[1, 2, 4, 5], [4, 3, 2, 9]])\n",
    "input[0][0] = -1\n",
    "embedding(input)\n",
    "input[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "train = list()\n",
    "with open('data/train.jsonl') as f:\n",
    "    for l in f.readlines():\n",
    "        data = json.loads(l.strip())\n",
    "        train.append(data['index'])\n",
    "with open('data/train.txt','w+') as f:\n",
    "    f.write('\\n'.join(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>commit_id</th>\n",
       "      <th>owner</th>\n",
       "      <th>repo</th>\n",
       "      <th>source</th>\n",
       "      <th>CVE_ID</th>\n",
       "      <th>CWE_ID</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>540958e2f5a87b81aa5f55ce40b3e2869754f97d</td>\n",
       "      <td>stoth68000</td>\n",
       "      <td>media-tree</td>\n",
       "      <td>wild</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>non-security</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                 commit_id       owner  \\\n",
       "0           0  540958e2f5a87b81aa5f55ce40b3e2869754f97d  stoth68000   \n",
       "\n",
       "         repo source CVE_ID CWE_ID      category  \n",
       "0  media-tree   wild    NaN    NaN  non-security  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "df = pd.read_csv('meta_patch_db.csv')\n",
    "df.head(1)\n",
    "type_dict = dict()\n",
    "for _,row in df.iterrows():\n",
    "    index= str(row['commit_id'])\n",
    "    index = index.lower()\n",
    "    type_dict[index] = 1 if row['category'] == 'security' else 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34294"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(type_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cms = list()\n",
    "with open('data/train.txt') as f:\n",
    "    cms.extend([l.strip().split('_')[-1] for l in f.readlines()])\n",
    "with open('data/test.txt') as f:\n",
    "    cms.extend([l.strip().split('_')[-1] for l in f.readlines()])\n",
    "with open('data/valid.txt') as f:\n",
    "    cms.extend([l.strip().split('_')[-1] for l in f.readlines()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6575"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(cms).difference(set(type_dict.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('pathtree.jsonl') as f:\n",
    "    paths = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3253\n"
     ]
    }
   ],
   "source": [
    "result = list()\n",
    "file = 'data/test.jsonl'\n",
    "c = 0\n",
    "with open(file) as f:\n",
    "    for l in f.readlines():\n",
    "        data = json.loads(l.strip())\n",
    "        index = data['index']\n",
    "        if index in paths.keys():\n",
    "            if paths[index] == \"\":\n",
    "                continue\n",
    "            data['path'] = paths[index]\n",
    "        else:\n",
    "            continue\n",
    "        result.append(json.dumps(data))\n",
    "with open(file+'2','w+') as f:\n",
    "    f.write('\\n'.join(result))\n",
    "print(len(result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
