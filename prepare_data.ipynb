{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'cmg-data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of commits: 32153\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "df = pd.read_parquet(f'{DATA_DIR}/cmg-data-processed.parquet', engine='fastparquet')\n",
    "print(f'Num of commits:', df['index'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 23172   |   Val size: 2575   |   Test size: 6406\n"
     ]
    }
   ],
   "source": [
    "def get_id(dir_path='cmg-data/split-data', type='randomly'):\n",
    "    with open(f'{dir_path}/{type}/train_id.txt') as file:\n",
    "        train_val_id = [line.rstrip() for line in file]\n",
    "    with open(f'{dir_path}/{type}/test_id.txt') as file:\n",
    "        test_id = [line.rstrip() for line in file]\n",
    "\n",
    "    df = pd.DataFrame(train_val_id, columns=['index'])\n",
    "    train_percent = 0.9\n",
    "    train_id = list(df['index'].sample(int(df['index'].nunique() * train_percent), random_state=42))\n",
    "    val_id = df[~df['index'].isin(train_id)]['index']\n",
    "\n",
    "    print(f'Training size: {len(train_id)}   |   Val size: {len(val_id)}   |   Test size: {len(test_id)}')\n",
    "    \n",
    "    return train_id, val_id, test_id\n",
    "\n",
    "train_id, val_id, test_id = get_id(dir_path=f'{DATA_DIR}/split-data', type='cross_project')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_type = pd.read_csv('meta_patch_db.csv')\n",
    "type_dict = dict()\n",
    "for _,row in df_type.iterrows():\n",
    "    index = str(row['commit_id'])\n",
    "    index = index.lower()\n",
    "    type_dict[index] = 1 if row['category'] == 'security' else 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32153/32153 [01:44<00:00, 306.51it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32153"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "data = list()\n",
    "index_list = set(df['index'])\n",
    "\n",
    "for id in tqdm(index_list):\n",
    "    df_commit = df[df['index']==id]\n",
    "    codes = list()\n",
    "    for _, row in df_commit .iterrows():\n",
    "        diff = row['change_abstract']\n",
    "        msg = row['msg_change_abstract'].split()\n",
    "        if row['old_path_file'] == row['new_path_file']:\n",
    "            file_name = row['new_path_file']\n",
    "        else:\n",
    "            if row['old_path_file'] is not None and row['new_path_file'] is not None:\n",
    "                file_name = row['old_path_file'] + ' SEP ' + row['new_path_file']\n",
    "            else:\n",
    "                file_name = row['old_path_file'] if row['old_path_file'] is not None else row['new_path_file']\n",
    "\n",
    "        code = file_name + ' SEP ' + diff\n",
    "        code = code.split() + [\"SEP\"]\n",
    "        codes.extend(code)\n",
    "    commit_id = id.split('_')[-1]\n",
    "\n",
    "    if commit_id in type_dict.keys():\n",
    "        type = type_dict[commit_id]\n",
    "    else:\n",
    "        type = 1\n",
    "\n",
    "    data.append({'code_tokens': codes, \n",
    "                 'docstring_tokens': msg, \n",
    "                 'index': id.replace('_file_fc_patch.csv',''), \n",
    "                 'type': type\n",
    "                })\n",
    "    \n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test,val = [],[],[]\n",
    "for el in data:\n",
    "    if el['index'] in train_id:\n",
    "        train.append(el)\n",
    "    elif el['index'] in val_id:\n",
    "        val.append(el)\n",
    "    else:\n",
    "        test.append(el)\n",
    "\n",
    "import json\n",
    "def dump_to_file(obj, file):\n",
    "    with open(file, 'w') as f:\n",
    "        for el in obj:\n",
    "            f.write(json.dumps(el)+'\\n')\n",
    "\n",
    "dump_to_file(train,f'{DATA_DIR}/train.jsonl')\n",
    "dump_to_file(test,f'{DATA_DIR}/test.jsonl')\n",
    "dump_to_file(val,f'{DATA_DIR}/valid.jsonl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
